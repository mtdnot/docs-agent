import os
import re
from dotenv import load_dotenv
from github_loader import all_docs
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA

load_dotenv()

# æ–‡æ›¸ã‚’åˆ†å‰²
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
split_docs = splitter.split_documents(all_docs)

# TSXãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ«ãƒ¼ãƒˆå®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ç´¢
def find_possible_routes(base_dir):
    routes = set()
    for root, _, files in os.walk(base_dir):
        for file in files:
            if file.endswith(".tsx"):
                path = os.path.join(root, file)
                with open(path, "r", encoding="utf-8") as f:
                    content = f.read()
                if "<Route" in content or "useRoutes(" in content or "createBrowserRouter" in content:
                    routes.add(path)
    return routes

# App.tsxã‹ã‚‰ãƒ«ãƒ¼ãƒˆã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆæŠ½å‡º
def extract_routed_components_from_app(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        content = f.read()

    route_targets = set()
    route_pattern = re.compile(r"element=\{<([A-Z]\w*)")
    for match in route_pattern.finditer(content):
        component_name = match.group(1)
        route_targets.add(component_name)

    return route_targets

# ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç‰¹å®š
def resolve_component_to_file(component_name, search_root="src"):
    for root, _, files in os.walk(search_root):
        for file in files:
            if file.endswith(".tsx") and file.removesuffix(".tsx").endswith(component_name):
                return os.path.join(root, file)
    return None

print("--- DEBUGGING ---")

# ãƒ«ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¢ã™
#possible_routes = find_possible_routes("src/components/")
possible_routers = "src/components/App.tsx"
print(f"Found {len(possible_routes)} route files: {possible_routes}")
# routed_componentsæŠ½å‡º
routed_components = set()
print(f"Found {len(routed_components)} component names: {routed_components}")
for route_file in possible_routes:
    routed_components.update(extract_routed_components_from_app(route_file))

# ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã«å¤‰æ›
routed_files = set()
for comp in routed_components:
    path = resolve_component_to_file(comp)
    if path:
        routed_files.add(path)
routed_files.update(possible_routes)  # ãƒ«ãƒ¼ãƒˆå®šç¾©ãƒ•ã‚¡ã‚¤ãƒ«ã‚‚å¯¾è±¡
print(f"Resolved {len(routed_files)} file paths: {routed_files}")

# å¯¾è±¡ãƒšãƒ¼ã‚¸ã‚’çµã‚Šè¾¼ã¿
page_docs = [doc for doc in split_docs if doc.metadata.get("path", "") in routed_files]
print(f"Number of page_docs: {len(page_docs)}")
# ãƒ™ã‚¯ãƒˆãƒ«DBä½œæˆ
vectordb = Chroma.from_documents(page_docs, OpenAIEmbeddings(), persist_directory="vectorstore")

# Retriever + LLM
retriever = vectordb.as_retriever(search_kwargs={"k": 5})
llm = ChatOpenAI(model="gpt-4", temperature=0.4)
qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever)

# TOC èª­ã¿è¾¼ã¿
with open("output/toc.md", "r", encoding="utf-8") as f:
    toc_lines = f.readlines()

# ã‚»ã‚¯ã‚·ãƒ§ãƒ³æŠ½å‡º
sections = []
for line in toc_lines:
    if line.startswith("## "):
        sections.append(line.strip("#\n ").strip())
    elif re.match(r"^\s*-\s+\[.+\]\(.+\)", line):
        match = re.match(r"^\s*-\s+\[(.+?)\]\(.+\)", line)
        if match:
            sections.append(match.group(1))

# ç”Ÿæˆ
os.makedirs("output/sections", exist_ok=True)
for section in sections:
    if not section:
        continue
    print(f"ğŸ“ ã‚»ã‚¯ã‚·ãƒ§ãƒ³: {section}")
    result = qa_chain.invoke(f"ã€{section}ã€ã«ã¤ã„ã¦ã€è£½å“MCP-Routerã«å³ã—ã¦ã€åˆ©ç”¨è€…ã®è¦–ç‚¹ã‹ã‚‰å¿…è¦ãªæƒ…å ±ã®ã¿ã‚’ç°¡æ½”ã«èª¬æ˜ã—ã¦ãã ã•ã„ï¼ˆ200å­—ä»¥å†…ï¼‰")
    with open(f"output/sections/{section.replace('/', '_')}.md", "w", encoding="utf-8") as f:
        f.write(f"## {section}\n\n{result}")

print("âœ… å…¨ç« ã®æœ¬æ–‡ç”ŸæˆãŒå®Œäº†ï¼output/sections/ ã‚’ç¢ºèªã—ã¦ã­")
